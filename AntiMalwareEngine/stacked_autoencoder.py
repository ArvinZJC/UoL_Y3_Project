'''
@Description: a stacked autoencoder
@Version: 1.1.6.20200331
@Author: Robin Nix and Jian Zhang
@Date: 2020-02-22 10:02:21
@Last Editors: Jichen Zhao
@LastEditTime: 2020-03-31 11:41:24
'''

import numpy as np
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # disable printing Tensorflow debugging messages with the level INFO
import pickle
import tensorflow as tf
import tensorflow_core.contrib.layers as lays

from decompressor import decompress


def compress(inputs, variable_list = None, is_saver_variable_list = False):
    '''
    Compress input data with optimisers.

    Parameters
    ----------
    inputs : input data

    variable_list : a list of variables to save (default: `None`)

    is_saver_variable_list : a boolean value indicating if the saver variable list is returned (default: `False`)

    Returns
    -------
    saver_variable_list (the value of `is_saver_variable_list` is `True`) : the saver variable list

    data_points (the value of `is_saver_variable_list` is `False`) : compressed input data
    '''

    input_size = 50
    learning_rate = 0.00001

    X = tf.placeholder(tf.float32, [None, input_size, 86796, 1])
    reconstruction, compressed, encoder_1, encoder_2, decoder_1, decoder_2 = autoencoder(X)
    loss_op = tf.reduce_mean(tf.square(reconstruction - X))
    optimiser = tf.train.AdamOptimizer(learning_rate = learning_rate)
    training_op = optimiser.minimize(loss_op)

    if is_saver_variable_list:
    	return tf.train.Saver()._var_list # add ops to save and restore all variables
    
    saver = tf.train.Saver(var_list = variable_list) # add ops to save and restore all variables
	
    with tf.Session() as session:
        init_op = tf.variables_initializer([variable for variable in tf.global_variables() if variable.name.split(':')[0] in set(session.run(tf.report_uninitialized_variables()))]) # add an op to initialise variables
        session.run(init_op)
        batch_x = decompress(inputs, 86796)
        [_, data_points] = session.run([reconstruction, compressed], feed_dict = {X: batch_x})
        
    return data_points


def autoencoder(inputs):
    '''
    Act as an autoencoder to process input data.

    Parameters
    ----------
    inputs : input data

    Returns
    -------
    reconstruction : part of result of the third (final) decoder (50 * 10850 * 32  ->  50 * 86796 * 1)
    compressed : result of the third (final) encoder (50 * 5825 * 16  ->  50 * 1357 * 8)
    encoder_1 : result of the first encoder (50 * 86796 * 1  ->  50 * 10850 * 32)
    encoder_2 : result of the second encoder (50 * 10850 * 32  ->  50 * 5825 * 16)
    decoder_1 : result of the first decoder (50 * 1357 * 8  ->  50 * 5825 * 16)
    decoder_2 : result of the second decoder (50 * 5825 * 16  ->  50 * 10850 * 32)
    '''

    # encoder
    encoder_1 = lays.conv2d(inputs, 32, [5, 5], stride = (1, 8), padding = 'same') # 50 * 86796 * 1  ->  50 * 10850 * 32
    encoder_2 = lays.conv2d(encoder_1, 16, [5, 5], stride = (1, 2), padding = 'same') # 50 * 10850 * 32  ->  50 * 5825 * 16
    compressed = lays.conv2d(encoder_2, 8, [5, 5], stride = (1, 4), padding = 'same') # 50 * 5825 * 16  ->  50 * 1357 * 8

    # decoder
    decoder_1 = lays.conv2d_transpose(compressed, 16, [5, 5], stride = (1, 4), padding = 'same') # 50 * 1357 * 8  ->  50 * 5825 * 16
    decoder_2 = lays.conv2d_transpose(decoder_1, 32, [5, 5], stride = (1, 2), padding = 'same') # 50 * 5825 * 16  ->  50 * 10850 * 32
    decoder_3 = lays.conv2d_transpose(decoder_2, 1, [5, 5], stride = (1, 8), padding = 'same', activation_fn = tf.nn.tanh) # 50 * 10850 * 32  ->  50 * 86796 * 1

    return decoder_3[:, :, 0 : inputs.get_shape().as_list()[2], :], compressed, encoder_1, encoder_2, decoder_1, decoder_2


# to train a CNN with a stacked autoencoder, the following code needs running successfully first
if __name__ == '__main__':
    from data_reader import load_training_data
    from logger import Logger
    from path_loader import get_ae_saver_path, get_data_directory
    
    learning_rate = 0.00001
    epoch_count = 1
    batch_size = 1
    display_step = 1
    dimension_count = 86796
    input_size = 50
   
    X = tf.placeholder(tf.float32, [None, input_size, dimension_count, 1])
    reconstruction, compressed, encoder_1, encoder_2, decoder_1, decoder_2 = autoencoder(X)
    loss_op = tf.reduce_mean(tf.square(reconstruction - X))

    tf.summary.scalar('loss', loss_op)
    optimiser = tf.train.AdamOptimizer(learning_rate = learning_rate)
    training_op = optimiser.minimize(loss_op)
    
    init_op = tf.global_variables_initializer() # add an op to initialise variables
    data_X, _ = load_training_data()
    indices = np.random.permutation(np.arange(data_X.shape[0]))
    
    data_X = data_X[indices, :, :]

    lays.summarize_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    merged = tf.summary.merge_all()
    saver = tf.train.Saver() # add ops to save and restore all variables

    config = tf.ConfigProto(allow_soft_placement = True)
    config.gpu_options.allow_growth = True
    
    with tf.Session(config = config) as session:
        training_writer = tf.summary.FileWriter(get_data_directory(), session.graph)
        session.run(init_op)

        i = 0
        
        for epoch in range(epoch_count):
            for step in range(int(data_X.shape[0] / batch_size)):
                batch_x = data_X[step * batch_size : (step + 1) * batch_size]
                i += 1
                
                batch_x = decompress(batch_x, dimension_count)
                _, summary = session.run([training_op, merged], feed_dict = {X: batch_x})
                training_writer.add_summary(summary, i)

                # calculate batch loss
                if step % display_step == 0:
                    loss = session.run(loss_op, feed_dict = {X: batch_x})
                    Logger(os.path.basename(__file__), __name__).i('Epoch: ' + str(epoch)
                        + ', step: ' + str(step)
                        + ', loss: ' + '{:.4f}'.format(loss)) # initialise a logger and add a message with the level INFO

        saver.save(session, get_ae_saver_path())