'''
@Description: TODO:
@Version: 1.0.0.20200222
@Author: Arvin Zhao
@Date: 2020-02-22 10:02:21
@Last Editors: Arvin Zhao
@LastEditTime: 2020-02-22 14:36:36
'''

import numpy as np
import os
import pickle
import tensorflow as tf
import tensorflow.contrib.layers as lays # TODO:

from data_reader import load_data
from decompressor import decompress


def print_intermediate():
    '''
    TODO:
    '''

    learning_rate = 0.00001
    num_epoch = 1
    batch_size = 1
    display_step = 1
    input_size = 50
   
    X = tf.placeholder(tf.float32, [None, input_size, 86796, 1])
    reconstruction, compressed, encoder_1, encoder_2, decoder_1, decoder_2 = autoencoder(X)
    loss_op = tf.reduce_mean(tf.square(reconstruction - X))
    tf.summary.scalar('loss', loss_op)
    init = tf.global_variables_initializer()
    data_X, _ = load_data()
    indices = np.random.permutation(np.arange(data_X.shape[0]))
    data_X = data_X[indices, :, :]
    lays.summarize_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    merged = tf.summary.merge_all()
    saver = tf.train.Saver()

    with tf.Session() as sess:
        train_writer = tf.summary.FileWriter('ae_logs_1/', sess.graph)
        sess.run(init) # run the initialiser

        i = 0
        
        for epoch in range(num_epoch):
            for step in range(data_X.shape[0] / batch_size):
                batch_x = data_X[step * batch_size : (step + 1) * batch_size]

                i += 1

                batch_x = decompress(batch_x, 86796)
                saver.restore(sess, 'ae_logs_1/save.ckpt')
                init_new_op = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in set(sess.run(tf.report_uninitialized_variables()))])
                sess.run(init_new_op)
                stage1, stage2, stage3 = sess.run([encoder_1, encoder_2, compressed], feed_dict = {X: batch_x})

                # exit(0) # ???


def autoencoder(inputs): # TODO:
    '''
    TODO:
    '''

    # encoder
    # 50 x 86796 x 1  ->  50 x 10850 x 32
    # 50 x 10850 x 32  ->  50 x 5825 x 16
    # 50 x 5825 x 16    ->  50 x 1357 x 8
    encoder_1 = lays.conv2d(inputs, 32, [5, 5], stride = (1, 8), padding = 'same')
    encoder_2 = lays.conv2d(encoder_1, 16, [5, 5], stride = (1, 2), padding = 'same')
    compressed = lays.conv2d(encoder_2, 8, [5, 5], stride = (1, 4), padding = 'same')

    # decoder
    # 50 x 1357 x 8    ->  50 x 5825 x 16 
    # 50 x 5825 x 16   ->  50 x 10850 x 32
    # 50 x 10850 x 32  ->   50 x 86796 x 1
    decoder_1 = lays.conv2d_transpose(compressed, 16, [5, 5], stride = (1, 4), padding = 'same')
    decoder_2 = lays.conv2d_transpose(decoder_1, 32, [5, 5], stride = (1, 2), padding = 'same')
    decoder_3 = lays.conv2d_transpose(decoder_2, 1, [5, 5], stride = (1, 8), padding = 'same', activation_fn = tf.nn.tanh)

    return decoder_3[:, :, 0 : inputs.get_shape().as_list()[2], :], compressed, encoder_1, encoder_2, decoder_1, decoder_2


def compress(input, x = None, return_x = False):
    '''
    TODO:
    '''

    input_size = 50
    learning_rate = 0.00001
    X = tf.placeholder(tf.float32, [None, input_size, 86796, 1])
    reconstruction, compressed, encoder_1, encoder_2, decoder_1, decoder_2 = autoencoder(X)
    loss_op = tf.reduce_mean(tf.square(reconstruction - X))
    optimiser = tf.train.AdamOptimizer(learning_rate = learning_rate)
    train_op = optimiser.minimize(loss_op)

    if return_x:
    	saver = tf.train.Saver()
    	x = saver._var_list

    	return x
    
    saver = tf.train.Saver(var_list = x)
	
    with tf.Session() as sess1:
        init_new_op = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in set(sess1.run(tf.report_uninitialized_variables()))])
        sess1.run(init_new_op)
        batch_x = decompress(input, 86796)
        [_, data_point] = sess1.run([reconstruction, compressed], feed_dict = {X: batch_x})
        
    return data_point


# test purposes only
if __name__ == '__main__':
    os.environ['CUDA_VISIBLE_DEVICES'] = ''

    print_intermediate()
    # exit(0) # ???

    learning_rate = 0.00001
    num_epoch = 1
    batch_size = 1
    display_step = 1
    input_size = 50
   
    X = tf.placeholder(tf.float32, [None, input_size, 86796, 1])
    reconstruction, compressed, encoder_1, encoder_2, decoder_1, decoder_2 = autoencoder(X)
    loss_op = tf.reduce_mean(tf.square(reconstruction - X))
    tf.summary.scalar('loss',loss_op)
    optimiser = tf.train.AdamOptimizer(learning_rate = learning_rate)
    train_op = optimiser.minimize(loss_op)
    init = tf.global_variables_initializer()
    data_X, _ = load_data()
    indices = np.random.permutation(np.arange(data_X.shape[0]))
    data_X = data_X[indices, :, :]
    lays.summarize_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    merged = tf.summary.merge_all()
    saver = tf.train.Saver()

    with tf.Session() as sess:
        train_writer = tf.summary.FileWriter('ae_logs_1/', sess.graph)
        sess.run(init) # run the initializer

        i = 0
        
        for epoch in range(num_epoch):
            for step in range(data_X.shape[0] / batch_size):
                batch_x = data_X[step * batch_size : (step + 1) * batch_size]

                i += 1

                batch_x = decompress(batch_x, 86796)
                _, summary = sess.run([train_op, merged], feed_dict = {X: batch_x})
                train_writer.add_summary(summary, i)

                if step % display_step == 0:
                    loss = sess.run(loss_op, feed_dict = {X: batch_x}) # calculate batch loss and accuracy

                if i % 20 == 0:
                    save_path = saver.save(sess, os.path.join('ae_logs_1', 'save.ckpt'))