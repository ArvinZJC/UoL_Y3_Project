'''
@Description: TODO:
@Version: 1.0.1.20200221
@Author: Arvin Zhao
@Date: 2020-02-20 19:01:12
@Last Editors: Arvin Zhao
@LastEditTime: 2020-02-21 13:22:46
'''

from androguard.core.analysis import analysis
from androguard.core.bytecodes import apk, dvm
import numpy as np
import os
import pickle
import re
import sys

from logger import Logger


max_h = 50
max_calls = 50


def extract_compressed_features():
    '''
    TODO: Extract all feature vectors for 600 apps in the dataset and pickle them.
    '''

    log = Logger(os.path.basename(__file__), sys._getframe().f_code.co_name)
    api_call_dictionary = pickle.load(open('common_dict_300.save', 'rb'))

    path_list = ['Dataset/all_drebin']
    index = 0

    for i in range(2):
        count = 0

        for path in os.listdir(path_list[i])[::-1]:
            count += 1
            
            if count == 34:
	            break
        
            index += 1

            try:
                x = get_compressed_feature_vector(os.path.join(path_list[i], path), api_call_dictionary)

                data_point = {}
                data_point['x'] = x
                data_point['y'] = 1
                
                fp = open(os.path.join('acf2', str(path) + '.save'), 'wb')
                pickle.dump(data_point, fp, protocol = pickle.HIGHEST_PROTOCOL)
                fp.close()	
            except:
                log.e('An exception occurred.')


def get_compressed_feature_vector(path, api_call_dictionary):
    '''
    TODO:
    '''

    feature_vector = np.zeros((max_calls, max_h), dtype = int)

    call_count = 0
    sequence_count = 0

    if path.endswith('.apk'):
        app = apk.APK(path)
        app_dex = dvm.DalvikVMFormat(app.get_dex())
    else: 
        app_dex = dvm.DalvikVMFormat(open(path, 'rb').read())

    app_x = analysis.newVMAnalysis(app_dex)
    cs = [cc.get_name() for cc in app_dex.get_classes()]

    for method in app_dex.get_methods():
        g = app_x.get_method(method)
    
        if method.get_code() == None:
            continue

        for i in g.get_basic_blocks().get():
            if i.childs != [] and sequence_count < max_h:
                call_count = 0
                
                for ins in i.get_instructions():
                    output = ins.get_output()
                    match = re.search(r'(L[^;]*;)->[^\(]*\([^\)]*\).*', output)
                    
                    if match and match.group(1) not in cs and call_count < max_calls:
                        feature_vector[call_count, sequence_count] = api_call_dictionary[match.group()]
                        call_count += 1
                
                rand_child_selected = np.random.randint(len(i.childs))
                traverse_graph(i.childs[rand_child_selected][2], feature_vector, cs, call_count, sequence_count, api_call_dictionary)
                
                sequence_count += 1

    return feature_vector


def traverse_graph(node,
    feature_vector,
    cs,
    call_count,
    sequence_count,
    api_call_dictionary):
    '''
    TODO:
    '''

    for ins in node.get_instructions():
        output = ins.get_output()
        match = re.search(r'(L[^;]*;)->[^\(]*\([^\)]*\).*', output)

        if match and match.group(1) not in cs and call_count < max_calls:
            feature_vector[call_count, sequence_count] = api_call_dictionary[match.group()]
            call_count += 1
    
    if call_count < max_calls and node.childs != []:
        rand_child_selected = np.random.randint(len(node.childs))
        traverse_graph(node.childs[rand_child_selected][2], feature_vector, cs, call_count, sequence_count, api_call_dictionary)


# test purposes only
if __name__ == '__main__':
    extract_compressed_features()