'''
@Description: training a CNN with a stacked autoencoder
@Version: 1.1.6.20200331
@Author: Robin Nix and Jian Zhang
@Date: 2020-02-22 14:12:51
@Last Editors: Jichen Zhao
@LastEditTime: 2020-03-31 11:41:06
'''

import numpy as np
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # disable printing Tensorflow debugging messages with the level INFO
import tensorflow as tf

from cnn_ops import conv_net, get_one_hot_vector
from data_reader import load_training_data
from decompressor import decompress
from logger import Logger
from path_loader import get_ae_saver_path, get_cnn_trainer_saver_path, get_data_directory
from stacked_autoencoder import autoencoder, compress


saver_variable_list = compress(None, None, True)

learning_rate = 0.00001
epoch_count = 1
batch_size = 1
display_step = 1
dimension_count = 86796
input_size = 50
class_count = 2

X = tf.placeholder(tf.float32, [None, input_size, dimension_count, 1])
reconstruction, compressed, _, _, _, _ = autoencoder(X)
cnn_X = tf.placeholder(tf.float32, [None, input_size, 1357, 8])
cnn_Y = tf.placeholder(tf.float32, [None, class_count])
cnn_prediction = conv_net(cnn_X)
cnn_loss_op = tf.contrib.slim.losses.softmax_cross_entropy(cnn_prediction, cnn_Y) # use "slim" to make defining, training, and evaluating a CNN simple

tf.summary.scalar('loss', cnn_loss_op)
cnn_optimiser = tf.train.AdagradOptimizer(learning_rate = learning_rate)
cnn_training_op = cnn_optimiser.minimize(cnn_loss_op)

cnn_correct_prediction = tf.equal(tf.argmax(cnn_prediction, 1), tf.argmax(cnn_Y, 1))
cnn_accuracy = tf.reduce_mean(tf.cast(cnn_correct_prediction, tf.float32))
tf.summary.scalar('accuracy', cnn_accuracy)

cnn_init_op = tf.global_variables_initializer()
data_X, data_Y = load_training_data()
indices = np.random.permutation(np.arange(data_X.shape[0]))

data_X = data_X[indices, :, :]
data_Y = data_Y[indices]

cnn_merged = tf.summary.merge_all()
cnn_saver = tf.train.Saver()
saver = tf.train.Saver(var_list = saver_variable_list)
log = Logger(os.path.basename(__file__), __name__) # initialise a logger

with tf.Session() as session:
    cnn_training_writer = tf.summary.FileWriter(get_data_directory(), session.graph)
    session.run(cnn_init_op)
    saver.restore(session, get_ae_saver_path())

    i = 0
    training_accuracy = 0.0
    
    for epoch in range(epoch_count):
        for step in range(data_X.shape[0] / batch_size):
            batch_x, batch_y = data_X[step * batch_size : (step + 1) * batch_size], data_Y[step * batch_size : (step + 1) * batch_size]
            i += 1

            batch_x = decompress(batch_x, dimension_count)
            batch_x = session.run(compressed, feed_dict = {X: batch_x})
            batch_y = get_one_hot_vector(batch_size, batch_y)
            batch_y = np.repeat(batch_y, input_size, axis = 0)
            assert(batch_x.shape[0] == batch_y.shape[0])
            _, summary = session.run([cnn_training_op, cnn_merged], feed_dict = {cnn_X: batch_x, cnn_Y: batch_y})
            cnn_training_writer.add_summary(summary, i)

            if step % display_step == 0:
                loss, accuracy, summary = session.run([cnn_loss_op, cnn_accuracy, cnn_merged], feed_dict = {cnn_X: batch_x, cnn_Y: batch_y}) # calculate batch loss and accuracy
                training_accuracy += accuracy
                log.i('Epoch: ' + str(epoch)
                    + ', step: ' + str (step)
                    + ', loss: ' + '{:.4f}'.format(loss)
                    + ', accuracy: ' + '{:.3f}'.format(accuracy))
    
    cnn_saver.save(session, get_cnn_trainer_saver_path())
    log.i('Mean training accuracy: ' + '{:.3f}'.format(training_accuracy / i))