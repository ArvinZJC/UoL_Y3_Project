'''
@Description: TODO:
@Version: 1.0.1.20200325
@Author: Robin Nix and Jian Zhang
@Date: 2020-02-22 14:12:51
@Last Editors: Arvin Zhao
@LastEditTime: 2020-03-25 14:55:37
'''

import numpy as np
import os
import tensorflow as tf

from data_reader import load_data
from decompressor import decompress
from stacked_autoencoder import autoencoder, compress


slim = tf.contrib.slim


def lrelu(alpha):
    '''
    TODO:
    '''

    def op(inputs):
        '''
        TODO:
        '''
        return tf.maximum(alpha * inputs, inputs, name = 'leaky_relu')

    return op


def conv_net(input):
    '''
    TODO:
    '''

    with slim.arg_scope([slim.conv2d, slim.fully_connected],
        activation_fn = lrelu(0.005),
        weights_initializer = tf.truncated_normal_initializer(0.0, 0.01),
        weights_regularizer = slim.l2_regularizer(0.0005)): # using the scope to avoid mentioning the parameters repeatedly
        net = slim.conv2d(input, 512, (3, 1357), 1, padding = 'valid', scope = 'cnn_conv_1')
        net = slim.max_pool2d(net, (4, 1), 4, padding = 'valid', scope = 'cnn_pool_2')
        net = slim.conv2d(net, 512, (5, 1), 1, scope = 'cnn_conv_3')
        net = slim.max_pool2d(net, (4, 1), 4, padding = 'valid', scope = 'cnn_pool_4')
        net = slim.flatten(net, scope = 'cnn_flatten_5')
        net = slim.fully_connected(net, 2, scope = 'cnn_fc_8', activation_fn = tf.nn.softmax)

    return net


def one_hot(batch_size,Y):
    '''
    TODO:
    '''

    B = np.zeros((batch_size, 2))
    B[np.arange(batch_size), Y] = 1

    return B


# test purposes only
if __name__ == '__main__':
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'
    
    x = compress(None, None, True)

    learning_rate = 0.00001
    num_epoch = 1
    batch_size = 1
    display_step = 1
    input_size = 50
    num_classes = 2 

    X = tf.placeholder(tf.float32, [None, input_size, 86796, 1])
    reconstruction, compressed, _, _, _, _ = autoencoder(X)
    cnn_X = tf.placeholder(tf.float32, [None, input_size, 1357, 8])
    cnn_Y = tf.placeholder(tf.float32, [None, num_classes])
    cnn_prediction = conv_net(cnn_X)
    cnn_loss_op = slim.losses.softmax_cross_entropy(cnn_prediction, cnn_Y)
    tf.summary.scalar('loss', cnn_loss_op)
    cnn_optimizer = tf.train.AdagradOptimizer(learning_rate = learning_rate)
    cnn_train_op = cnn_optimizer.minimize(cnn_loss_op)
    cnn_correct_pred = tf.equal(tf.argmax(cnn_prediction, 1), tf.argmax(cnn_Y, 1))
    cnn_accuracy = tf.reduce_mean(tf.cast(cnn_correct_pred, tf.float32))
    tf.summary.scalar('accuracy', cnn_accuracy)
    cnn_init = tf.global_variables_initializer()
    data_X, data_Y = load_data()
    indices = np.random.permutation(np.arange(data_X.shape[0]))
    data_X = data_X[indices, :, :]
    data_Y = data_Y[indices]
    cnn_merged = tf.summary.merge_all()
    cnn_saver = tf.train.Saver()
    saver = tf.train.Saver(var_list = x)

    with tf.Session() as sess:
        cnn_train_writer = tf.summary.FileWriter('cnn_logs_ae/', sess.graph)
        sess.run(cnn_init)
        saver.restore(sess, 'ae_logs_1/save.ckpt')

        i = 0
        train_acc = 0.0
        
        for epoch in range(num_epoch):
            for step in range(data_X.shape[0] / batch_size):
                batch_x, batch_y = data_X[step * batch_size : (step + 1) * batch_size], data_Y[step * batch_size : (step + 1) * batch_size]

                i+=1

                batch_x = decompress(batch_x, 86796)
                batch_x = sess.run(compressed, feed_dict = {X: batch_x})
                batch_y = one_hot(batch_size, batch_y)
                batch_y = np.repeat(batch_y, 50, axis = 0)
                assert(batch_x.shape[0] == batch_y.shape[0])
                _, summary = sess.run([cnn_train_op, cnn_merged], feed_dict = {cnn_X: batch_x, cnn_Y: batch_y})
                cnn_train_writer.add_summary(summary, i)

                if step % display_step == 0:
                    loss, acc,summary = sess.run([cnn_loss_op, cnn_accuracy, cnn_merged], feed_dict = {cnn_X: batch_x, cnn_Y: batch_y}) # calculate batch loss and accuracy
                    
                    train_acc += acc
                
                if i % 20 == 0:
                    save_path = cnn_saver.save(sess, os.path.join('cnn_logs_ae', 'save.ckpt'))