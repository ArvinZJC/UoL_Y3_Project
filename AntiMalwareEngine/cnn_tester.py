'''
@Description: TODO:
@Version: 1.0.0.20200222
@Author: Arvin Zhao
@Date: 2020-02-22 14:12:59
@Last Editors: Arvin Zhao
@LastEditTime: 2020-02-22 21:28:21
'''

import numpy as np
import os
import tensorflow as tf

from data_reader import load_data
from decompressor import decompress
from stacked_autoencoder import autoencoder, compress


slim = tf.contrib.slim


def lrelu(alpha):
    '''
    TODO:
    '''

    def op(inputs):
        '''
        TODO:
        '''
        return tf.maximum(alpha * inputs, inputs, name = 'leaky_relu')

    return op


def conv_net(input):
    '''
    TODO:
    '''

    with slim.arg_scope([slim.conv2d, slim.fully_connected],
        activation_fn = lrelu(0.005),
        weights_initializer = tf.truncated_normal_initializer(0.0, 0.01),
        weights_regularizer = slim.l2_regularizer(0.0005)): # using the scope to avoid mentioning the parameters repeatedly
        net = slim.conv2d(input, 512, (3, 1357), 1, padding = 'valid', scope = 'cnn_conv_1')
        net = slim.max_pool2d(net, (4, 1), 4, padding = 'valid', scope = 'cnn_pool_2')
        net = slim.conv2d(net, 512, (5, 1), 1, scope = 'cnn_conv_3')
        net = slim.max_pool2d(net, (4, 1), 4, padding = 'valid', scope = 'cnn_pool_4')
        net = slim.flatten(net, scope = 'cnn_flatten_5')
        net = slim.fully_connected(net, 2, scope = 'cnn_fc_8', activation_fn = tf.nn.softmax)

    return net


def one_hot(batch_size,Y):
    '''
    TODO:
    '''

    B = np.zeros((batch_size, 2))
    B[np.arange(batch_size), Y] = 1

    return B


# test purposes only
if __name__ == '__main__':
    learning_rate = 0.00001
    num_epoch = 1
    batch_size = 1
    display_step = 1
    input_size = 50
    num_classes = 2 

    X = tf.placeholder(tf.float32, [None, input_size, 86796, 1])
    reconstruction, compressed, _, _, _, _ = autoencoder(X)
    cnn_X = tf.placeholder(tf.float32, [None, input_size, 1357, 8])
    cnn_Y = tf.placeholder(tf.float32, [None, num_classes])
    cnn_prediction = conv_net(cnn_X)
    cnn_loss_op = slim.losses.softmax_cross_entropy(cnn_prediction, cnn_Y)
    cnn_optimizer = tf.train.AdagradOptimizer(learning_rate = learning_rate)
    cnn_train_op = cnn_optimizer.minimize(cnn_loss_op)
    cnn_correct_pred = tf.equal(tf.argmax(cnn_prediction, 1), tf.argmax(cnn_Y, 1))
    cnn_accuracy = tf.reduce_mean(tf.cast(cnn_correct_pred, tf.float32))
    cnn_init = tf.global_variables_initializer()
    test_data_X, test_data_Y = load_data() # original function (load_test_data) not found

    with tf.Session() as sess:
        saver = tf.train.Saver()
        saver.restore(sess, 'cnn_logs_ae/save.ckpt')
        
        i = 0
        test_acc = 0.0

        for step in range(test_data_X.shape[0] / batch_size):
            batch_x, batch_y = test_data_X[step * batch_size : (step + 1) * batch_size], test_data_Y[step * batch_size : (step + 1) * batch_size]

            i+=1

            batch_x = decompress(batch_x, 86796)
            batch_x = sess.run(compressed, feed_dict = {X: batch_x})
            batch_y = one_hot(batch_size, batch_y)
            batch_y = np.repeat(batch_y, 50, axis = 0)
            assert(batch_x.shape[0] == batch_y.shape[0])
            acc = sess.run(cnn_accuracy, feed_dict = {cnn_X: batch_x,cnn_Y: batch_y})

            test_acc += acc